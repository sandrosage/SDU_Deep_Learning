{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Retain the sequential nature of the review\n",
    "    Beibehaltung des sequentiellen Charakters der Überprüfung\n",
    "\n",
    "### (2) Remove any linear relationships between the input data numbers\n",
    "    Entfernen Sie alle linearen Beziehungen zwischen den Zahlen der Eingabedaten\n",
    "\n",
    "### (3) Retain a consistent data size for a neural network input\n",
    "    Beibehaltung einer einheitlichen Datengröße für die Eingabe eines neuronalen Netzes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_words\n",
    "\n",
    "    integer or None. Words are ranked by how often they occur (in the training set) and only the num_words \n",
    "    most frequent words are kept. Any less frequent word will appear as oov_char value in the sequence data. \n",
    "    If None, all words are kept. Defaults to None, so all words are kept.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = imdb.load_data(num_words=1000)\n",
    "\n",
    "vocab = imdb.get_word_index()\n",
    "#print(vocab)        #{'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, 'sonja': 16816,...} key -> value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "maxlen\n",
    "\n",
    "\tOptional Int, maximum length of all sequences. If not provided, sequences will be padded to the \n",
    "\tlength of the longest individual sequence.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "\n",
    "train_x = pad_sequences(train_x, maxlen=max_words)\n",
    "test_x = pad_sequences(test_x, maxlen=max_words)\n",
    "\n",
    "embedding_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer arguments\n",
    "```python\n",
    "input_dim:      Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "\n",
    "output_dim:     Integer. Dimension of the dense embedding.\n",
    "\n",
    "input_length:   Length of input sequences, when it is constant. This argument is required if you are going \n",
    "                to connect Flatten then Dense layers upstream (without it, the shape of the dense \n",
    "                outputs cannot be computed).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 32)           32000     \n",
      "                                                                 \n",
      " simple_rnn_6 (SimpleRNN)    (None, 500, 100)          13300     \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, 500, 50)           7550      \n",
      "                                                                 \n",
      " simple_rnn_8 (SimpleRNN)    (None, 25)                1900      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 54,776\n",
      "Trainable params: 54,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Dense, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=1000, output_dim=embedding_size, input_length=(max_words)),\n",
    "    SimpleRNN(100, return_sequences=\"true\"),\n",
    "    SimpleRNN(50, return_sequences=\"true\"),\n",
    "    SimpleRNN(25),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 85s 531ms/step - loss: 0.7035 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.5110\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 78s 500ms/step - loss: 0.6366 - acc: 0.6187 - val_loss: 0.6338 - val_acc: 0.6954\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 77s 490ms/step - loss: 0.5199 - acc: 0.7505 - val_loss: 0.8845 - val_acc: 0.6134\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 78s 495ms/step - loss: 0.4984 - acc: 0.7661 - val_loss: 0.5741 - val_acc: 0.6844\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 78s 499ms/step - loss: 0.4795 - acc: 0.7804 - val_loss: 0.5026 - val_acc: 0.7642\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 77s 489ms/step - loss: 0.4750 - acc: 0.7796 - val_loss: 0.4993 - val_acc: 0.7570\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 76s 483ms/step - loss: 0.4564 - acc: 0.7998 - val_loss: 0.5345 - val_acc: 0.7862\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 76s 487ms/step - loss: 0.4701 - acc: 0.7916 - val_loss: 0.5665 - val_acc: 0.6990\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 76s 484ms/step - loss: 0.4669 - acc: 0.7916 - val_loss: 0.4307 - val_acc: 0.8196\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 76s 485ms/step - loss: 0.4517 - acc: 0.7972 - val_loss: 0.5275 - val_acc: 0.7268\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "history = model.fit(train_x,train_y, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "584da16cd62a9f533c0ac40541222fd213977bd98b4b3d472b8442495038f20f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
